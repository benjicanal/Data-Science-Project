{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780b93b4-038e-4550-bd27-4e88805bb7dc",
   "metadata": {},
   "source": [
    "# Face Recognition APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4ca43-7422-4743-b651-442cfe7077cb",
   "metadata": {},
   "source": [
    "Install usefull libraries : \n",
    "1. Labelme : Application pour permettre l'annotation d'images\n",
    "2. Open CV\n",
    "3. Albumentations : Bibliothèque pour l'augmentation des données dans la vision par ordinateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba839c-bf70-40a4-a299-a9ae1cebf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python tensorflow labelme albumentations matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "74687c69-8f5c-4ed4-be9a-3dba3332816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import cv2\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445627d-9a6f-4156-a033-f1b5ae77d42c",
   "metadata": {},
   "source": [
    "## 1. Récupération des données "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c8b85-6efc-4937-b8b7-20458bff7736",
   "metadata": {},
   "source": [
    "**Code utile pour récupérer les données de la web cam utilisant open cv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9c63c4e5-71f1-46bf-8eff-be20d3e01b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('data','images')\n",
    "number_images = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b1ad3be3-dbfa-4420-8b60-c31aba1a2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "for imgnum in range(number_images):\n",
    "    print('Collecting image {}'.format(imgnum))\n",
    "    ret, frame = cap.read()\n",
    "    imgname = os.path.join(IMAGES_PATH,f'{str(uuid.uuid1())}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "233d22ee-4471-47c2-9f12-6fa65a8640f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !labelme # Ouvrir l'application label me, ne pas oublier de changer le repertoire pour les outputs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff2913-120d-4b43-9bee-17fd6cbf86f6",
   "metadata": {},
   "source": [
    "## 2. Revu du jeu de données et construction d'une fonction pour charger les images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fd64515b-27f5-4304-94f4-cbd32dc94a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0c42c218-f44c-4d2c-8536-26b9181930fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus : \n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36945a-3d7f-4e6a-91ff-7d02c5e8bea8",
   "metadata": {},
   "source": [
    "### Charger les données en un tf data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3f37d635-d35d-4986-9ef6-47891c6df2ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: data\\\\images\\\\*.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Créer un jeu de données en spécifiant le dossier comportant nos images\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m*.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\face_recognition\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1320\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[1;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[0;32m   1313\u001b[0m condition \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mgreater(array_ops\u001b[38;5;241m.\u001b[39mshape(matching_files)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   1314\u001b[0m                              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_not_empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1316\u001b[0m message \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files matched pattern: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1318\u001b[0m     string_ops\u001b[38;5;241m.\u001b[39mreduce_join(file_pattern, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1320\u001b[0m assert_not_empty \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_assert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massert_not_empty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assert_not_empty]):\n\u001b[0;32m   1323\u001b[0m   matching_files \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(matching_files)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\face_recognition\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\face_recognition\\Lib\\site-packages\\tensorflow\\python\\ops\\control_flow_assert.py:102\u001b[0m, in \u001b[0;36mAssert\u001b[1;34m(condition, data, summarize, name)\u001b[0m\n\u001b[0;32m    100\u001b[0m     xs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_n_to_tensor(data)\n\u001b[0;32m    101\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m [_summarize_eager(x, summarize) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[0;32m    103\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    104\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    105\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be true. Summarized data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    106\u001b[0m         (condition, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_str)))\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssert\u001b[39m\u001b[38;5;124m\"\u001b[39m, [condition, data]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: data\\\\images\\\\*.jpg'"
     ]
    }
   ],
   "source": [
    "# Créer un jeu de données en spécifiant le dossier comportant nos images\n",
    "images = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2931f-8d92-4724-891b-253c6f7fa0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet de parcourir le tenseur images\n",
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e800a3-f7eb-4d09-80b2-a9a0980e7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x) : \n",
    "    byte_img = tf.io.read_file(x) #io pour input/output, module pour écrire, lire  ou charger fichier. byte_img est un tenseur d'octet\n",
    "    img = tf.io.decode_jpeg(byte_img) #décoder la suite d'octet en une image \n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af55ae3-5621-413b-b7dc-ad296321d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_image) # Permet de passer la fonction sur toutes les images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33053952-07fb-4a80-9d35-3d28ed14427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20516fbd-b7ca-4e39-bb7c-b0b799203724",
   "metadata": {},
   "source": [
    "### Visualiser nos images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46d93a-02d5-4533-be50-c50876d982b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_generator = images.batch(4).as_numpy_iterator() #batch permet de prendre plusieurs données en même temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a915b-06db-4458-9bfe-f6e7ecc5df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = images_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae98c6-146e-429c-8bc0-8307b0223f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols = 4, figsize = (28,20)) # Créer une figure avec un tableau d'axes de 4 colonnes\n",
    "for idx, image in enumerate(plot_images)  : \n",
    "    ax[idx].imshow(image) # Place l'image 1 sur l'axe 0 etc...\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b27fd7-3097-4c8d-bf8b-89d91646ba26",
   "metadata": {},
   "source": [
    "## 3. Séparer les données en train / test / val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da427f6-776d-404b-b1c8-3644ff42cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90 * 0.7 pour le train set = 63\n",
    "# 90 * 0.15 pour le test et val = 14 et 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd4074-54eb-4dce-83e4-8931548e2181",
   "metadata": {},
   "source": [
    "Une fois que l'on a bougé les images dans les dossiers correspondants, on doit également bouger les labels associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee4de1-d8a2-4f5e-a6c7-0bf90f1dca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce code permet de changer les labels du dossiers data\\label aux nouveaux dossiers correspondants\n",
    "for folder in ['train','test','val'] : \n",
    "    for file in os.listdir(os.path.join('data',folder,'images')) : \n",
    "        filename = file.split('.')[0]+'.json' \n",
    "        existing_filepath = os.path.join('data','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224987e5-1f10-4e87-a3f7-bd582cafd7a4",
   "metadata": {},
   "source": [
    "# 4. Augmenter les images et les labels avec Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d315fb-c12b-49dd-a8fa-bb75d6523f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e334eb9-e12c-4f60-bdaf-4b87ea263d12",
   "metadata": {},
   "source": [
    "Pour augmenter nos images et labels, on utilise la librairie albumentations. Tout d'abord on crée une pipeline des transformations que l'on veut effectuer avec l'objet 'alb.Compose' en faisant passer une liste de toutes les transformations que l'on désire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7be6f4-8fe5-4e0c-8a36-cb6d2a59bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ef597-2104-488e-8a38-a899bc8977bb",
   "metadata": {},
   "source": [
    "Petit test sur une image du train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee64038-4322-49ae-9883-a65aad24c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data','train','images','582357ab-5040-11ef-b567-8b544a84180a.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8470b-6ebe-4898-8dd5-67a634199fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comme vu sur coursera , on charge un fichier json et 'r' pour lire le fichier\n",
    "with open(os.path.join('data','train','labels','582357ab-5040-11ef-b567-8b544a84180a.json'),'r') as l : \n",
    "    label = json.load(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41aeb1-4ed5-47cc-bf6b-4e5548cdde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a425594-fac7-4727-aef5-c6165783933a",
   "metadata": {},
   "source": [
    "On récupère maintenant les coordonnées du label sur une seule liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e066f-d5af-407c-b5b0-2019ce43db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83431f-e0ca-40a5-86cc-3faf73bf095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][0]['points'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f7b27-8044-40fa-8a9c-5ba70c2d9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = [0,0,0,0]\n",
    "coord[0] = label['shapes'][0]['points'][0][0]\n",
    "coord[1] = label['shapes'][0]['points'][0][1]\n",
    "coord[2] = label['shapes'][0]['points'][1][0]\n",
    "coord[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466830a-0fa0-44a0-9c50-7346ed8663fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34cafdc-3ebc-424f-ac8d-2607f72ca30a",
   "metadata": {},
   "source": [
    "On normalise maintenant nos coordonnées \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f6084-d5d3-49de-9a38-c336fe53b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_norm = list(np.divide(coord, [640,480,640,480]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b7bb2-8cfe-46e4-ace3-763195760f52",
   "metadata": {},
   "source": [
    "**On peut désormais regarder nos augmentations sur l'image et son label associé**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58449328-1352-4f7c-a7b1-cd773d2266b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image=img, bboxes=[coord_norm], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc4c0e-09ec-4bbc-bb15-ea0567f0d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16da387-6a18-4faf-84c9-d16582f459e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'], \n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)), \n",
    "                    (255,0,0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69283873-f0c1-4393-aae3-df7e8e1dccdc",
   "metadata": {},
   "source": [
    "## 5. Créer la pipeline d'augmentations des données sur toutes nos images / labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d1965-9d6d-4701-ba58-14262c1f15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [640,480,640,480]))\n",
    "\n",
    "        try: \n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d37617-1eb9-4117-b304-37340977b432",
   "metadata": {},
   "source": [
    "**Maintenant il faut créer les datasets train / test / val avec les nouvelles images augmentées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f486d74-3140-4b67-b65c-2bf7fecb78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle = False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120))) #Pour améliorer notre réseau de neuronnes\n",
    "train_images = train_images.map(lambda x : x/255) #Normaliser les images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27137e11-3285-46b3-a857-fa1fbdca5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle = False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x : x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44dc037-b119-4145-aef3-7f2a594f18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle = False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x : x/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed865a19-b37d-4bbf-894e-fb249aac3fcd",
   "metadata": {},
   "source": [
    "## 6. Préparer les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f453fc7-3b06-475b-9f26-543be0f3f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f: #convertit le tenseur en chaine de carac\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbf287-1fc6-40b7-85da-e0a03e9835bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16])) # py_function pour utiliser des fonctions pythons arbitraires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703a804-b399-47e9-97f0-afa3c991ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56579d-9378-41e4-8829-41a03438cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae338e-2046-4cd1-ba47-5f8a4613413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231e547-b527-4738-89f2-642481d552c2",
   "metadata": {},
   "source": [
    "## 7. Combiner l'image et le label en 1 échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003950f-0dde-431b-95a6-eb3d9fd15e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_images), len(test_images), len(val_images), len(train_labels), len(test_labels), len(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda361fa-fdf0-4402-9b53-d7b28d22f78b",
   "metadata": {},
   "source": [
    "**On crée maintenant nos datasets finaux composés des images et de leurs labels associés**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b7d9e-2a69-48aa-9188-f0e1db329a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels)) # Combine les images et les labels en 1 tuple\n",
    "train = train.shuffle(5000) # Représente le buffer de mélange (doit être >len(train_images) pour éviter la généralisation\n",
    "train = train.batch(8) # regroupe en des lots de 8 \n",
    "train = train.prefetch(4) # Permet de pré charger 4 lots en mémoire, ce qui améliore l'efficacité de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2021f-3761-4e9f-bf6d-a404b05c95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bc9fb-5ec2-405a-8fc4-93156a1cb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Assure-toi que les formes sont correctes\n",
    "print(train_images.shape)  # Par exemple, (num_samples, height, width, channels)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c556e30-04a2-470a-a888-f1090583fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "# Shuffle, batch, et prefetch\n",
    "train_dataset = train_dataset.shuffle(buffer_size=5000)  # Le buffer_size doit être >= len(train_images)\n",
    "train_dataset = train_dataset.batch(8)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2410f-a752-4317-81c5-a45b08f7149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc552be-6b90-4d63-98c8-79024aeac1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e938ea-8893-4a4a-bc18-ab3a045df6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ca3d4-70fc-4b57-92a4-06f03750cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_samples.next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d60bc-e922-49c7-a7a8-9dd617536fb5",
   "metadata": {},
   "source": [
    "**On peut afficher nos images avec les cadres associés** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572361f-a096-4a7f-950c-898def9fffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx].copy()\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3fdac-4438-4e8a-a959-ba279a57545b",
   "metadata": {},
   "source": [
    "## 8. Construire le model de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74888b48-f602-483e-bd6c-3217ca12c83b",
   "metadata": {},
   "source": [
    "On importe d'abord les modules nécessaires de l'API Keras \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3351cd-0496-443a-93f3-dd53ab8836e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Add, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771e52c-0842-4ca8-9c8e-fb6c13344474",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top = False) # Include top False car on veut personnaliser le model\n",
    "# préfabriqué avec notre problème de classification / regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ee6a7-5f2b-4f29-a433-46cfa8bdb579",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary() # commande pour voir l'architecture du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbd1bb-b19f-406f-92f1-95968c17b335",
   "metadata": {},
   "source": [
    "**On peut maintenant crée le modèle qui nous permettra de faire notre face recognition, et on commence par l architecture du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4843694-10fa-4df2-866a-355e59a388b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    # On crée la couche d'entrée qui est une image rgb de 120*120\n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "    # On utilise le réseau pré entrainé vgg16\n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "    # Maintenant il faut séparer notre problème en deux sous problèmes : \n",
    "    # Classification (face ou pas) et régression (estimé les positions du cadre)\n",
    "    \n",
    "    # Classification Model\n",
    "    # On ajoute un globalmaxpooling (couramment utilisé à la fin du CNN\n",
    "    # Pour réduire rapidement les dimensions de l'image\n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    # Sigmoid car c'est une classification binaire (0 ou 1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding box model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    # 4 unité car on a 4 coordonnées pour le cadre \n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    # On crée le modèle final avec le module Model(input, output)\n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952bb78-ef9c-47e9-8f26-e324e58227f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1dc877-e8fe-4926-b685-520a5abea2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259188df-e0de-4c41-a211-abbf1cfa280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acb536-180f-4f4b-83b7-7f37ec4808ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c5cc9-02d4-464e-aea9-9b408a466186",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de814f-cf39-4ec2-b1e4-8bc291f10ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142dd94-7f39-40a7-a382-165c098515d0",
   "metadata": {},
   "source": [
    "## 9. Définir le coût et la fonction d'optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6cbf7-f1b3-4d7c-8ad8-10ddbc2d2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = len(train)\n",
    "lr_decay = (1./0.75 -1) / BATCH # Pour baisser le learning rate progressivement afin de converger plus vite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e51498-d851-4b83-bdb9-52f19614dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Crée maintenant notre fonction d'optimisation \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e9900-f94d-447a-be94-254d4d3d84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss pour notre régression pour les cadres \n",
    "def localization_loss(y_true, yhat):            \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63735d1-fc13-44af-a1a5-305a8beb9261",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy() #classification binaire\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe6a1a-565f-4241-9df8-84fa1762dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7faf85-71b8-4f79-a515-421ba7e45e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b388263-22b6-484a-b1fd-759d0c3288ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1], coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e27ae1df-9dda-4aa4-8cf2-c70e5f97345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model): \n",
    "    def __init__(self, facetracker,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = facetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs): \n",
    "        print(batch)\n",
    "        X, y = batch\n",
    "        print(X, y)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "            print(\"Classes shape:\", classes.shape)\n",
    "            print(\"Coords shape:\", coords.shape)\n",
    "            print(\"y[0] shape (class labels):\", y[0].shape)\n",
    "            print(\"y[1] shape (bounding boxes):\", y[1].shape)\n",
    "\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1b88d3dd-587a-4029-a547-60d01d3f2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e4758e4e-4f14-4559-9513-08f346c9e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classloss, regressloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "01fdc683-8534-4c10-94b8-b4a42e6a7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e10cce8d-2fa3-4a89-b2c8-fc367f858936",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c822e7-9af4-44fc-baa4-95e44ede00d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7475326e-3db0-47c8-b926-ed724c9d40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "(<tf.Tensor 'data:0' shape=(None, 120, 120, None) dtype=float32>, (<tf.Tensor 'data_1:0' shape=<unknown> dtype=uint8>, <tf.Tensor 'data_2:0' shape=<unknown> dtype=float16>))\n",
      "Tensor(\"data:0\", shape=(None, 120, 120, None), dtype=float32) (<tf.Tensor 'data_1:0' shape=<unknown> dtype=uint8>, <tf.Tensor 'data_2:0' shape=<unknown> dtype=float16>)\n",
      "Classes shape: (None, 1)\n",
      "Coords shape: (None, 4)\n",
      "y[0] shape (class labels): <unknown>\n",
      "y[1] shape (bounding boxes): <unknown>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take the length of shape with unknown rank.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[237], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\face_recognition\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[232], line 25\u001b[0m, in \u001b[0;36mFaceTracker.train_step\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my[0] shape (class labels):\u001b[39m\u001b[38;5;124m\"\u001b[39m, y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my[1] shape (bounding boxes):\u001b[39m\u001b[38;5;124m\"\u001b[39m, y[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 25\u001b[0m batch_classloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m batch_localizationloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlloss(tf\u001b[38;5;241m.\u001b[39mcast(y[\u001b[38;5;241m1\u001b[39m], tf\u001b[38;5;241m.\u001b[39mfloat32), coords)\n\u001b[0;32m     28\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m batch_localizationloss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_classloss\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take the length of shape with unknown rank."
     ]
    }
   ],
   "source": [
    "hist = model.fit(train, epochs=40, validation_data=val, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "68bb788d-5089-4852-9227-1ea5ca046539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (8, 120, 120, 3)\n",
      "Class labels shape: (8, 1)\n",
      "Bounding box coordinates shape: (8, 4)\n"
     ]
    }
   ],
   "source": [
    "for batch in train.take(1):\n",
    "    images, (class_labels, bbox_coords) = batch\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Class labels shape:\", class_labels.shape)\n",
    "    print(\"Bounding box coordinates shape:\", bbox_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb61f1-262e-4587-8890-841bc82d5da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Face App Project",
   "language": "python",
   "name": "face_recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
